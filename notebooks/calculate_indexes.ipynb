{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import modules.network_extractor as net_extractor\n",
    "from osmnx import settings\n",
    "from modules.indices import graph_based as g_index\n",
    "from osmnx import simplification\n",
    "#from modules import generalize as generalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change for your own data base path\n",
    "data_base_path = \"/home/geolab/Desktop/Research/data\"\n",
    "\n",
    "# The extractor instance\n",
    "extractor = net_extractor.NetworkExtractor()\n",
    "extractor.DATA_BASE_PATH = data_base_path\n",
    " \n",
    "# Custom OSMnx settings\n",
    "settings.default_crs = \"epsg:4326\"\n",
    "\n",
    "# Generalizer instance\n",
    "#generalizer = generalize.Generalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the city name in lowercase and slug_case where the graphs are located\n",
    "city_name = \"sydney\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load graphs if they are already \n",
    "g_walk = extractor.load_graph(f'{city_name}/graph/walk_{city_name}')\n",
    "g_bike = extractor.load_graph(f'{city_name}/graph/bike_{city_name}')\n",
    "g_drive = extractor.load_graph(f'{city_name}/graph/drive_{city_name}')\n",
    "g_public = extractor.load_graph(f'{city_name}/graph/public_{city_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pedestrian graph for sydney\n",
      "374896\n",
      "298766\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pedestrian graph for {city_name}\")\n",
    "print(g_walk.number_of_edges())\n",
    "print(g_walk.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import networkx as nx\n",
    "from shapely import ops, LineString\n",
    "from itertools import chain\n",
    "import shapely\n",
    "import osmnx\n",
    "import networkx as nx\n",
    "from shapely.geometry import MultiLineString\n",
    "from shapely.ops import linemerge\n",
    "\n",
    "ANGLE_THRESHOLD = 10\n",
    "\n",
    "def grid_candidate(node_id, g: nx.MultiDiGraph | nx.MultiGraph):\n",
    "\n",
    "    preselect = False\n",
    "    if g.is_directed():\n",
    "        incoming_degree = g.in_degree(node_id)\n",
    "        outgoing_degree = g.out_degree(node_id)\n",
    "        if incoming_degree + outgoing_degree == 8: # 4 incoming and 4 outgoing \n",
    "            preselect = True\n",
    "    else:\n",
    "        incoming_degree = g.degree(node_id)\n",
    "        outgoing_degree = 0\n",
    "        if incoming_degree + outgoing_degree == 4: # 4 edges\n",
    "            preselect = True\n",
    "            \n",
    "    if preselect:\n",
    "        incident_edges = list(g[node_id].keys())\n",
    "\n",
    "        for neighboor_id in incident_edges:\n",
    "            edge_data = g.get_edge_data(node_id, neighboor_id)\n",
    "            road_type = \"\" if \"highway\" not in edge_data else edge_data[\"highway\"]\n",
    "            try:\n",
    "                length = None if \"length\" not in edge_data else float(edge_data[\"length\"])\n",
    "            except Exception as ex: length = None\n",
    "\n",
    "            if road_type is None or length is None: \n",
    "                return False\n",
    "            \n",
    "            if road_type != \"residential\" or length >= 300:\n",
    "                return False\n",
    "\n",
    "def is_insterstitial(node_id, g: nx.MultiDiGraph | nx.MultiGraph):\n",
    "    # for directed, an interstitial node has 1 incomming and 1 outgoing edges\n",
    "    if g.is_directed():\n",
    "        if g.out_degree(node_id) == 1 and g.in_degree(node_id) == 1:\n",
    "            return True\n",
    "        \n",
    "    # for undirected, an interstitial node exactly 2 edges\n",
    "    else:\n",
    "        if g.degree(node_id) == 2:\n",
    "            return True\n",
    "    \n",
    "    # in any othe cases, return false\n",
    "    return False\n",
    "\n",
    "def topology_preservation_generalization(input_graph: nx.MultiGraph | nx.MultiDiGraph, max_iterations=10):\n",
    "    \"\"\"\n",
    "    Based on the generalization methodology propsed in DOI: 10.1007/s41109-022-00521-8 .\n",
    "\n",
    "    It is an iterative algorithm that generalizes a traffic network by eliminating parallel edges,\n",
    "    self-loops, dead-ends, low-level gridirons, and interstitial nodes (to be implemented).\n",
    "\n",
    "    Works both for directed and undirected graphs.\n",
    "    \"\"\"\n",
    "\n",
    "    input_g = copy.deepcopy(input_graph)\n",
    "\n",
    "    is_modified = True\n",
    "    iterations = 0\n",
    "\n",
    "    # add aggr_node_number property to all nodes\n",
    "    for node_id in input_g.nodes():\n",
    "        input_g._node[node_id][\"aggr_node_number\"] = 1\n",
    "\n",
    "    print(f\"Initial: {input_g}\")\n",
    "    # Iterate when the graph has been modified (it has not yet converged) or the max iterations has been reached.\n",
    "    while is_modified and iterations <= max_iterations:\n",
    "        is_modified = False\n",
    "        iterations += 1\n",
    "        print(f\"iteration {iterations}\")\n",
    "        print(f\"Starting Nodes: {input_g.number_of_nodes()}\")\n",
    "        print(f\"Starting Edges: {input_g.number_of_edges()}\")\n",
    "        # 1) Parallel edges\n",
    "        delete_edges = []\n",
    "        for node in input_g.nodes():\n",
    "            for node_to in input_g[node]:\n",
    "\n",
    "                # select the shortest parallel edge\n",
    "                if len(list(input_g[node][node_to].keys())) > 1:\n",
    "                    min_parallel = 99999999\n",
    "                    min_parallel_key = -1\n",
    "                    for parallel_edge_key in input_g[node][node_to].keys():\n",
    "                        data = input_g.get_edge_data(node, node_to, parallel_edge_key, default=None)\n",
    "                        if data[\"length\"] > min_parallel:\n",
    "                            min_parallel = data[\"length\"]\n",
    "                            min_parallel_key = parallel_edge_key\n",
    "                        \n",
    "                    for parallel_edge_key in input_g[node][node_to].keys():\n",
    "                        if parallel_edge_key != min_parallel_key:\n",
    "                            delete_edges.append([node, node_to, parallel_edge_key])\n",
    "\n",
    "        if len(delete_edges) > 0:\n",
    "            is_modified = True\n",
    "            print(\"removing parallel edges\")\n",
    "            input_g.remove_edges_from(delete_edges)\n",
    "\n",
    "\n",
    "        # 2) Self loops\n",
    "        delete_edges = []\n",
    "        for u,v,key in input_g.edges(keys=True):\n",
    "            if u == v:\n",
    "                # self loop identified\n",
    "                delete_edges.append([u,v,key])\n",
    "                \n",
    "        if len(delete_edges) > 0:\n",
    "            is_modified = True\n",
    "            print(\"removing self loops\")\n",
    "            input_g.remove_edges_from(delete_edges)\n",
    "        \n",
    "        # 3) Simplify dead ends\n",
    "        delete_candidates: dict[any,set] = {}\n",
    "        # first pass to select candidates\n",
    "        for node_id in input_g.nodes():\n",
    "            incident_edges = input_g[node_id]\n",
    "            if len(incident_edges) <= 1:\n",
    "                    delete_candidates[node_id] = set()\n",
    "        \n",
    "        # second pass to count edges arriving to the candidates\n",
    "        for node_id in input_g.nodes():\n",
    "            # Exiting from nodes\n",
    "            incident_edges = input_g[node_id]\n",
    "            for v_edge in incident_edges:\n",
    "                if v_edge in delete_candidates:\n",
    "                    delete_candidates[v_edge].add(node_id)\n",
    "                if node_id in delete_candidates:\n",
    "                    delete_candidates[node_id].add(v_edge)\n",
    "        \n",
    "        # If the node is only accessed once, remove it\n",
    "        delete_nodes = []\n",
    "        for node_id, related_nodes in delete_candidates.items():\n",
    "            if len(related_nodes) <= 1:\n",
    "                delete_nodes.append(node_id)\n",
    "\n",
    "        if len(delete_nodes) > 0:\n",
    "            is_modified = True\n",
    "            print(\"removing dead ends\")\n",
    "            for delete_node in delete_nodes:\n",
    "                incident_edges = input_g[delete_node]\n",
    "                for v_edge in incident_edges:\n",
    "                    input_g._node[v_edge][\"aggr_node_number\"] += input_g._node[delete_node][\"aggr_node_number\"]\n",
    "                input_g.remove_node(delete_node)\n",
    "\n",
    "        # 4) Simplify gridiron structures\n",
    "        # candidates = []\n",
    "        # candidates = set()\n",
    "        # for node_id in input_g.nodes():\n",
    "        #     if grid_candidate(node): candidates.add(node_id)\n",
    "\n",
    "        # print(f\"Candidates: {len(candidates)}\")\n",
    "        # print(\"finished processing initial candidates\")\n",
    "        # # add the I cases to the candidates\n",
    "        # # for u,v in input_g.edges():\n",
    "        # #     print(u,v)\n",
    "        # #     incident_edges_u = list(input_g[u])\n",
    "        # #     incident_edges_v = list(input_g[v])\n",
    "        # #     # if all incident nodes of the edge are in the gridiron, add it as well\n",
    "        # #     if incident_edges_u in candidates and incident_edges_v in candidates:\n",
    "        # #         candidates.add(u)\n",
    "        # #         candidates.add(v)\n",
    "        \n",
    "        # # print(\"Finished 'I' special cases\")\n",
    "        # print(\"removing grid\")\n",
    "        # # Aggregate clusters of grids individually to propagate and distribute nodes\n",
    "        # while len(candidates) > 0:\n",
    "        #     candidate = candidates.pop()\n",
    "\n",
    "        #     remove_candidates = [candidate]\n",
    "        #     gridiron_entrances = []\n",
    "        #     aggregated = 0\n",
    "        #     visit = [candidate]\n",
    "        #     visited = set()\n",
    "        #     # recursively visit the individual grid to extract the aggregated value\n",
    "        #     while True:\n",
    "        #         if len(visit) == 0: break # end condition\n",
    "        #         current_node = visit.pop()\n",
    "        #         if current_node in visited: continue # already visited node\n",
    "        #         visited.add(current_node)\n",
    "        #         remove_candidates.append(current_node)\n",
    "        #         aggregated += input_g._node[current_node][\"aggr_node_number\"]\n",
    "        #         current_edges = list(input_g[current_node])\n",
    "        #         for edg in current_edges:\n",
    "        #             if edg in candidates and edg not in visited:\n",
    "        #                 visit.append(edg)\n",
    "        #             elif edg not in candidates and edg not in visited:\n",
    "        #                 visited.add(edg)\n",
    "        #                 gridiron_entrances.append(edg)\n",
    "            \n",
    "        #     # obtained the data from a single grid element recursively.\n",
    "        #     if len(gridiron_entrances) > 0:\n",
    "        #         distributed_node_agg = aggregated / len(gridiron_entrances)\n",
    "\n",
    "        #     # distribute node aggregate value to gridiron entrances\n",
    "        #     for entrance in gridiron_entrances:\n",
    "        #         input_g._node[entrance][\"aggr_node_number\"] += distributed_node_agg\n",
    "\n",
    "        #     # remove the nodes from candidates and the graph\n",
    "        #     for to_remove in remove_candidates:\n",
    "        #         if to_remove in candidates:\n",
    "        #             candidates.remove(to_remove)\n",
    "        #         if input_g.has_node(to_remove):\n",
    "        #             input_g.remove_node(to_remove)\n",
    "        #             is_modified = True\n",
    "\n",
    "        # 5) Remove interstitial nodes  \n",
    "        #input_copy = input_g.copy()\n",
    "        nodes_to_check = list(input_g.nodes())\n",
    "\n",
    "        # for each endpoint node, look at each of its successor nodes\n",
    "        for node_id in nodes_to_check:\n",
    "            if not is_insterstitial(node_id, input_g):\n",
    "                continue\n",
    "\n",
    "            neighbors = list(input_g.neighbors(node_id))\n",
    "\n",
    "            if len(neighbors) != 2:\n",
    "                continue  # Just a sanity check\n",
    "\n",
    "            #print(neighbors, node_id)\n",
    "            n1, n2 = neighbors\n",
    "\n",
    "            if input_g.has_edge(node_id, n1):\n",
    "                u = n1\n",
    "                v = n2\n",
    "            else: \n",
    "                u = n2\n",
    "                v = n1\n",
    "\n",
    "\n",
    "            # Get edge geometries\n",
    "            keys = list(input_g.get_edge_data(node_id, u).keys())\n",
    "            if len(keys) > 1:\n",
    "                continue\n",
    "            \n",
    "            edge_1 = list(input_g.get_edge_data(node_id, u).values())[0]\n",
    "            edge_2 = list(input_g.get_edge_data(v, node_id).values())[0]\n",
    "            \n",
    "            # Combine geometries\n",
    "            merged_geom = linemerge(MultiLineString([edge_1['geometry'], edge_2['geometry']]))\n",
    "            combined_length = edge_1[\"length\"] + edge_2[\"length\"]\n",
    "\n",
    "            # Remove node and its edges\n",
    "            removed_node_count = input_g._node[node_id][\"aggr_node_number\"]\n",
    "            input_g.remove_node(node_id)\n",
    "\n",
    "            # update neighbours node counts - sum 0.5 nodes to each node\n",
    "            input_g._node[u][\"aggr_node_number\"] += (removed_node_count / 2)\n",
    "            input_g._node[v][\"aggr_node_number\"] += (removed_node_count / 2)\n",
    "\n",
    "            # Add new edge if not already present\n",
    "            if input_g.has_edge(u, v):\n",
    "                continue\n",
    "            \n",
    "            new_edge_data = {}\n",
    "            for attribute in edge_1.keys():\n",
    "                if attribute == \"geometry\":\n",
    "                    new_edge_data[\"geometry\"] = merged_geom\n",
    "                elif attribute == \"length\":\n",
    "                    new_edge_data[\"length\"] = combined_length\n",
    "                else:\n",
    "                    if attribute not in edge_1:\n",
    "                        edge_1[attribute] = None\n",
    "                    if attribute not in edge_2:\n",
    "                        edge_2[attribute] = None\n",
    "\n",
    "                    if edge_1[attribute] == edge_2[attribute]:\n",
    "                        new_edge_data[attribute] = edge_1[attribute]\n",
    "                    elif type(edge_1[attribute]) == list:\n",
    "                        new_edge_data[attribute] = edge_1[attribute].append(edge_2[attribute])\n",
    "                    elif type(edge_2[attribute]) == list:\n",
    "                        new_edge_data[attribute] = edge_2[attribute].append(edge_1[attribute])\n",
    "                    else:\n",
    "                        new_edge_data[attribute] = edge_1[attribute]\n",
    "                        \n",
    "                #TODO manage other attributes\n",
    "                \n",
    "            input_g.add_edge(u, v, **new_edge_data)\n",
    "\n",
    "        print(f\"Ending Nodes: {input_g.number_of_nodes()}\")\n",
    "        print(f\"Ending Edges: {input_g.number_of_edges()}\")\n",
    "\n",
    "    # Return the simplified graph\n",
    "    print(f\"Final: {input_g}\")\n",
    "    return input_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple = named_streets_generalization(g_walk, is_directed=False)\n",
    "#extractor.save_as_shp(simple, f'{city_name}/shp/simplified_named_{city_name}', line_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geolab/Desktop/Research/.venv/lib/python3.11/site-packages/osmnx/simplification.py:585: UserWarning: Geometry is in a geographic CRS. Results from 'buffer' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  merged = gdf_nodes.buffer(tolerance).union_all()\n",
      "/home/geolab/Desktop/Research/.venv/lib/python3.11/site-packages/osmnx/simplification.py:640: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  centroids = node_clusters.centroid\n"
     ]
    }
   ],
   "source": [
    "from modules import utils\n",
    "tolerance = 10 / utils.DEG_CONVERT\n",
    "merged_intersections = osmnx.simplification.consolidate_intersections(g_drive, tolerance=tolerance, rebuild_graph=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: MultiGraph with 298766 nodes and 374896 edges\n",
      "iteration 1\n",
      "Starting Nodes: 298766\n",
      "Starting Edges: 374896\n",
      "removing parallel edges\n",
      "removing dead ends\n",
      "Ending Nodes: 139424\n",
      "Ending Edges: 203453\n",
      "iteration 2\n",
      "Starting Nodes: 139424\n",
      "Starting Edges: 203453\n",
      "removing dead ends\n",
      "Ending Nodes: 117507\n",
      "Ending Edges: 181065\n",
      "iteration 3\n",
      "Starting Nodes: 117507\n",
      "Starting Edges: 181065\n",
      "removing dead ends\n",
      "Ending Nodes: 114219\n",
      "Ending Edges: 177535\n",
      "iteration 4\n",
      "Starting Nodes: 114219\n",
      "Starting Edges: 177535\n",
      "removing dead ends\n",
      "Ending Nodes: 113536\n",
      "Ending Edges: 176741\n",
      "iteration 5\n",
      "Starting Nodes: 113536\n",
      "Starting Edges: 176741\n",
      "removing dead ends\n",
      "Ending Nodes: 113285\n",
      "Ending Edges: 176449\n",
      "iteration 6\n",
      "Starting Nodes: 113285\n",
      "Starting Edges: 176449\n",
      "removing dead ends\n",
      "Ending Nodes: 113189\n",
      "Ending Edges: 176329\n",
      "iteration 7\n",
      "Starting Nodes: 113189\n",
      "Starting Edges: 176329\n",
      "removing dead ends\n",
      "Ending Nodes: 113152\n",
      "Ending Edges: 176281\n",
      "iteration 8\n",
      "Starting Nodes: 113152\n",
      "Starting Edges: 176281\n",
      "removing dead ends\n",
      "Ending Nodes: 113134\n",
      "Ending Edges: 176260\n",
      "iteration 9\n",
      "Starting Nodes: 113134\n",
      "Starting Edges: 176260\n",
      "removing dead ends\n",
      "Ending Nodes: 113130\n",
      "Ending Edges: 176255\n",
      "iteration 10\n",
      "Starting Nodes: 113130\n",
      "Starting Edges: 176255\n",
      "removing dead ends\n",
      "Ending Nodes: 113126\n",
      "Ending Edges: 176250\n",
      "iteration 11\n",
      "Starting Nodes: 113126\n",
      "Starting Edges: 176250\n",
      "removing dead ends\n",
      "Ending Nodes: 113124\n",
      "Ending Edges: 176247\n",
      "iteration 12\n",
      "Starting Nodes: 113124\n",
      "Starting Edges: 176247\n",
      "Ending Nodes: 113122\n",
      "Ending Edges: 176245\n",
      "Final: MultiGraph with 113122 nodes and 176245 edges\n"
     ]
    }
   ],
   "source": [
    "general = topology_preservation_generalization(g_walk, max_iterations=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u\n",
      "v\n",
      "key\n",
      "geometry\n",
      "length\n",
      "highway\n",
      "footway\n",
      "bearing\n",
      "foot\n",
      "speed_kph\n",
      "travel_time\n",
      "grade\n",
      "grade_abs\n",
      "name\n",
      "access\n",
      "sidewalk\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geolab/Desktop/Research/notebooks/modules/network_extractor.py:279: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  nodes.to_file(f\"{self.DATA_BASE_PATH}/{path}_nodes.shp\", encoding='utf-8')\n",
      "/home/geolab/Desktop/Research/.venv/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'street_count' to 'street_cou'\n",
      "  ogr_write(\n",
      "/home/geolab/Desktop/Research/.venv/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'aggr_node_number' to 'aggr_node_'\n",
      "  ogr_write(\n",
      "/home/geolab/Desktop/Research/notebooks/modules/network_extractor.py:281: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  edges.to_file(f\"{self.DATA_BASE_PATH}/{path}_edges.shp\", encoding='utf-8')\n",
      "/home/geolab/Desktop/Research/.venv/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'travel_time' to 'travel_tim'\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "extractor.save_as_shp(general, f'{city_name}/shp/simplified_topo_{city_name}')\n",
    "#extractor.save_as_shp(merged_intersections, f'{city_name}/shp/merged_intersections_{city_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u\n",
      "v\n",
      "key\n",
      "geometry\n",
      "length\n",
      "name\n",
      "highway\n",
      "sidewalk\n",
      "bearing\n",
      "foot\n",
      "footway\n",
      "speed_kph\n",
      "travel_time\n",
      "grade\n",
      "grade_abs\n",
      "access\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/geolab/Desktop/Research/notebooks/modules/network_extractor.py:279: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  nodes.to_file(f\"{self.DATA_BASE_PATH}/{path}_nodes.shp\", encoding='utf-8')\n",
      "/home/geolab/Desktop/Research/.venv/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'street_count' to 'street_cou'\n",
      "  ogr_write(\n",
      "/home/geolab/Desktop/Research/.venv/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'aggr_node_number' to 'aggr_node_'\n",
      "  ogr_write(\n",
      "/home/geolab/Desktop/Research/notebooks/modules/network_extractor.py:281: UserWarning: Column names longer than 10 characters will be truncated when saved to ESRI Shapefile.\n",
      "  edges.to_file(f\"{self.DATA_BASE_PATH}/{path}_edges.shp\", encoding='utf-8')\n",
      "/home/geolab/Desktop/Research/.venv/lib/python3.11/site-packages/pyogrio/raw.py:723: RuntimeWarning: Normalized/laundered field name: 'travel_time' to 'travel_tim'\n",
      "  ogr_write(\n"
     ]
    }
   ],
   "source": [
    "extractor.save_as_shp(general, f'{city_name}/shp/simplified_topo_{city_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
