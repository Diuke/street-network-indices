{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import scipy\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pyproj\n",
    "import networkx as nx\n",
    "import osmnx as ox\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as geopd\n",
    "import shapely\n",
    "from geopy import distance as geopy_distance\n",
    "from shapely import ops, LineString\n",
    "import requests\n",
    "from itertools import chain\n",
    "from modules import utils\n",
    "from networkx import bfs_edges\n",
    "from modules import generalize\n",
    "import sys\n",
    "sys.setrecursionlimit(50000)\n",
    "print(sys.getrecursionlimit())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modules.network_extractor as net_ex\n",
    "from modules.networker import Networker\n",
    "extractor = net_ex.NetworkExtractor()\n",
    "extractor.DATA_BASE_PATH = \"/home/user/Desktop/JP/street-network-indices/data\"\n",
    "networker = Networker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract information from the GHS dataset\n",
    "urban_centers: geopd.GeoDataFrame\n",
    "\n",
    "urban_centers = geopd.read_file(\n",
    "    \"/home/user/Desktop/JP/street-network-indices/data/GHS_URBAN_CENTERS/GHS_URBAN_SIMPLIFIED_fixed.gpkg\",\n",
    "    layer='GHS_URBAN_SIMPLIFIED_fixed'    \n",
    ")\n",
    "urban_centers = urban_centers.rename(columns={\n",
    "    \"GC_POP_TOT_2025\": \"population\",\n",
    "    \"GC_UCA_KM2_2025\": \"area\",\n",
    "    \"GC_DEV_USR_2025\": \"continent\",\n",
    "    \"GC_UCN_MAI_2025\": \"name\",\n",
    "    \"GC_CNT_GAD_2025\": \"country\"\n",
    "})\n",
    "transform = pyproj.Transformer.from_crs(\"ESRI:54009\", \"EPSG:4326\", always_xy=True).transform\n",
    "\n",
    "city_name = \"quito\"\n",
    "search_name = \"Quito\"\n",
    "country = \"Ecuador\"\n",
    "\n",
    "\n",
    "# extract info from GHS\n",
    "city_info = urban_centers.loc[\n",
    "    (urban_centers[\"name\"] == search_name) &\n",
    "    (urban_centers[\"country\"] == country)\n",
    "]\n",
    "\n",
    "geom = city_info[\"geometry\"].values[0]\n",
    "geom = ops.transform(transform, geom)\n",
    "\n",
    "# Load graphs\n",
    "g_walk = extractor.load_graph(f'{city_name}/graph/walk_{city_name}')\n",
    "g_bike = extractor.load_graph(f'{city_name}/graph/bike_{city_name}')\n",
    "g_drive = extractor.load_graph(f'{city_name}/graph/drive_{city_name}')\n",
    "g_public = extractor.load_graph(f'{city_name}/graph/public_{city_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create small buffer to include potential nodes outside\n",
    "tiles_buffer = 200 # meters\n",
    "buffer_distance = tiles_buffer / utils.DEG_CONVERT\n",
    "buffered = geom.buffer(buffer_distance)\n",
    "\n",
    "# Get bounds of the polygon\n",
    "minx, miny, maxx, maxy = buffered.bounds\n",
    "\n",
    "# Generate grid points\n",
    "tile_size = 0.005 # degrees\n",
    "x_coords = np.arange(minx, maxx, tile_size)\n",
    "y_coords = np.arange(miny, maxy, tile_size)\n",
    "\n",
    "tiles = {}\n",
    "i = 0\n",
    "j = 0\n",
    "for x in x_coords:\n",
    "    for y in y_coords:\n",
    "        tile_geom = shapely.geometry.box(x, y, x + tile_size, y + tile_size)\n",
    "        if geom.intersects(tile_geom):  # Keep only tiles that intersect original polygon\n",
    "            tiles[f\"{i},{j}\"] = {\n",
    "                \"row\": i,\n",
    "                \"column\": j,\n",
    "                \"min_x\": x,\n",
    "                \"max_x\": x + tile_size,\n",
    "                \"min_y\": y,\n",
    "                \"max_y\": y + tile_size,\n",
    "                \"geometry\": tile_geom,\n",
    "                \"nodes\": [],\n",
    "                \"centroid\": tile_geom.centroid\n",
    "            }\n",
    "        \n",
    "        j += 1\n",
    "    i += 1\n",
    "\n",
    "for node_id, data in g_walk.nodes(data=True):\n",
    "    x = data[\"x\"]\n",
    "    y = data[\"y\"]\n",
    "    node_tile = list(filter(lambda tile: x >= tile[\"min_x\"] and x <= tile[\"max_x\"] and y >= tile[\"min_y\"] and y <= tile[\"max_y\"], tiles.values()))\n",
    "    if len(node_tile) != 1:\n",
    "        # if not found, iterate over all tiles and select the closer \n",
    "        min_dist = np.inf\n",
    "        min_tile = None\n",
    "        for key,tile in tiles.items():\n",
    "            dist_to_centroid = utils.distance([x,y], [tile[\"centroid\"].x, tile[\"centroid\"].y])\n",
    "            if dist_to_centroid < min_dist:\n",
    "                min_dist = dist_to_centroid\n",
    "                min_tile = key\n",
    "        tiles[min_tile][\"nodes\"].append(node_id)\n",
    "\n",
    "    else:\n",
    "        tile = node_tile[0]\n",
    "        idx = f\"{tile['row']},{tile['column']}\"\n",
    "        tiles[idx][\"nodes\"].append(node_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_tile(tiles, x, y):\n",
    "    node_tile = list(filter(lambda tile: x >= tile[\"min_x\"] and x <= tile[\"max_x\"] and y >= tile[\"min_y\"] and y <= tile[\"max_y\"], tiles.values()))\n",
    "    if len(node_tile) != 1:\n",
    "        # if not found, iterate over all tiles and select the closer\n",
    "        min_dist = np.inf\n",
    "        min_tile = None\n",
    "        for key,tile in tiles.items():\n",
    "            dist_to_centroid = utils.distance([x,y], [tile[\"centroid\"].x, tile[\"centroid\"].y])\n",
    "            if dist_to_centroid < min_dist:\n",
    "                min_dist = dist_to_centroid\n",
    "                min_tile = key\n",
    "        idx = min_tile\n",
    "\n",
    "    else:\n",
    "        tile = node_tile[0]\n",
    "        idx = f\"{tile['row']},{tile['column']}\"\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(tiles, x, y, k):\n",
    "    distances = []\n",
    "    for key,tile in tiles.items():\n",
    "        distances.append({\n",
    "            \"key\": key,\n",
    "            \"dist\": utils.distance([x,y], [tile[\"centroid\"].x, tile[\"centroid\"].y])\n",
    "        })\n",
    "    \n",
    "    distances = sorted(distances, key=lambda d: d['dist'])\n",
    "    distances = distances[0:k]\n",
    "    return distances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished copy\n",
      "Finished adding nodes and edges\n",
      "Finished calculations\n",
      "588.9896082078641\n"
     ]
    }
   ],
   "source": [
    "DEG_CONVERT = 111595.75 \n",
    "threshold_distance = 300 # meters\n",
    "buffer_distance = threshold_distance / DEG_CONVERT\n",
    "\n",
    "# sampled_graph = g_public.subgraph(sampled_nodes)\n",
    "public_nodes_n = g_public.number_of_nodes()\n",
    "pubic_nodes_list = list(g_public.nodes())\n",
    "walk_public = copy.deepcopy(g_walk)\n",
    "print(f\"Finished copy\")\n",
    "\n",
    "first = True\n",
    "for node_id, data in g_public.nodes(data=True):\n",
    "\n",
    "    #nearest = knn(tiles, data[\"x\"], data[\"y\"], 2)\n",
    "    nearest = in_tile(tiles, data[\"x\"], data[\"y\"])\n",
    "    \n",
    "    walk_public.add_node(node_id, **data)\n",
    "    public_point = shapely.Point([data[\"x\"], data[\"y\"]])\n",
    "    public_point_buffer = utils.buffer_point(public_point, buffer_distance)\n",
    "\n",
    "    subsample_nodes = tiles[nearest][\"nodes\"]\n",
    "\n",
    "    min_dist = np.inf\n",
    "    min_node = None\n",
    "    data_xy = [data[\"x\"], data[\"y\"]]\n",
    "    for walk_node_id in subsample_nodes:\n",
    "        walk_data = g_walk._node[walk_node_id]\n",
    "\n",
    "        dist = utils.distance(data_xy, [walk_data[\"x\"], walk_data[\"y\"]])\n",
    "        if dist < min_dist:\n",
    "            min_dist = dist\n",
    "            min_node = walk_node_id\n",
    "    \n",
    "    walk_public.add_edge(node_id, min_node, **{\"length\": dist})\n",
    "\n",
    "print(f\"Finished adding nodes and edges\")\n",
    "all = nx.multi_source_dijkstra_path_length(walk_public, list(g_public.nodes()), weight=\"length\")\n",
    "mean_len = np.array(list(all.values())).mean()\n",
    "print(f\"Finished calculations\")\n",
    "print(mean_len)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: '5.0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_as_shp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwalk_public\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcity_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/shp/walk_public\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mcity_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Research/notebooks/modules/network_extractor.py:177\u001b[0m, in \u001b[0;36mNetworkExtractor.save_as_shp\u001b[0;34m(self, g, path, save_nodes, save_edges, line_graph)\u001b[0m\n\u001b[1;32m    175\u001b[0m     edges[col] \u001b[38;5;241m=\u001b[39m edges[col]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mint16)\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeed_kph\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 177\u001b[0m     edges[col] \u001b[38;5;241m=\u001b[39m \u001b[43medges\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint16\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m col \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtravel_time\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    179\u001b[0m     edges[col] \u001b[38;5;241m=\u001b[39m edges[col]\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/Desktop/Research/.venv/lib/python3.11/site-packages/pandas/core/generic.py:6643\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6637\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   6638\u001b[0m         ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   6639\u001b[0m     ]\n\u001b[1;32m   6641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6642\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6643\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6644\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m   6645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Research/.venv/lib/python3.11/site-packages/pandas/core/internals/managers.py:430\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    428\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Research/.venv/lib/python3.11/site-packages/pandas/core/internals/managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/Desktop/Research/.venv/lib/python3.11/site-packages/pandas/core/internals/blocks.py:758\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not squeeze with more than one column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    756\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    762\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Research/.venv/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:237\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    234\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/Research/.venv/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:182\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    179\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/Desktop/Research/.venv/lib/python3.11/site-packages/pandas/core/dtypes/astype.py:133\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: '5.0'"
     ]
    }
   ],
   "source": [
    "extractor.save_as_shp(walk_public, f'{city_name}/shp/walk_public{city_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With sampling\n",
    "# \n",
    "DEG_CONVERT = 111595.75 \n",
    "threshold_distance = 200 # meters\n",
    "buffer_distance = threshold_distance / DEG_CONVERT\n",
    "\n",
    "public_nodes_n = g_public.number_of_nodes()\n",
    "ks = [\n",
    "    int(public_nodes_n * 0.1), # 10%\n",
    "    int(public_nodes_n * 0.2), # 20%\n",
    "    int(public_nodes_n * 0.3), # 30%\n",
    "    int(public_nodes_n * 0.4), # 40%\n",
    "    int(public_nodes_n * 0.5), # 50%\n",
    "    int(public_nodes_n * 0.6), # 60%\n",
    "    int(public_nodes_n * 0.7), # 70%\n",
    "    int(public_nodes_n * 0.8), # 80%\n",
    "    int(public_nodes_n * 0.9), # 90%\n",
    "]\n",
    "\n",
    "for k in ks:\n",
    "    walk_public = copy.deepcopy(g_walk)\n",
    "    sampled_nodes = random.sample(list(g_public.nodes), k)\n",
    "    sampled_graph = g_public.subgraph(sampled_nodes)\n",
    "\n",
    "    print(f\"K={k} - starting adding nodes to graph\")\n",
    "    first = True\n",
    "    for node_id, data in sampled_graph.nodes(data=True):\n",
    "        walk_public.add_node(node_id, **data)\n",
    "\n",
    "        public_point = shapely.Point([data[\"x\"], data[\"y\"]])\n",
    "        public_point_buffer = utils.buffer_point(public_point, buffer_distance)\n",
    "\n",
    "        for walk_node_id, walk_data in g_walk.nodes(data=True):\n",
    "            within_distance = shapely.intersects_xy(public_point_buffer, walk_data[\"x\"], walk_data[\"y\"])\n",
    "            if within_distance:\n",
    "                dist = utils.distance([data[\"x\"], data[\"y\"]], [walk_data[\"x\"], walk_data[\"y\"]])\n",
    "                walk_public.add_edge(node_id, walk_node_id, **{\"length\": dist})\n",
    "        \n",
    "        if first: print(\"finish 1\")\n",
    "        first = False\n",
    "\n",
    "    print(f\"Finished adding nodes\")\n",
    "    \n",
    "    all = nx.multi_source_dijkstra_path_length(walk_public, sampled_nodes, weight=\"length\")\n",
    "    mean_len = np.array(list(all.values())).mean()\n",
    "    print(f\"K={k}\")\n",
    "    print(mean_len)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [\n",
    "1162.089300796976,\n",
    "837.4930336142239,\n",
    "724.5027779501886,\n",
    "647.0119264220768,\n",
    "597.7451792200699,\n",
    "537.6517238468184,\n",
    "538.1734136584903,\n",
    "500.2920286385221,\n",
    "468.17196463057627\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "K=150\n",
    "1162.089300796976\n",
    "\n",
    "K=301\n",
    "837.4930336142239\n",
    "\n",
    "K=452\n",
    "724.5027779501886\n",
    "\n",
    "K=603\n",
    "647.0119264220768\n",
    "\n",
    "K=754\n",
    "597.7451792200699\n",
    "\n",
    "K=905\n",
    "537.6517238468184\n",
    "\n",
    "K=1056\n",
    "538.1734136584903\n",
    "\n",
    "K=1207\n",
    "500.2920286385221\n",
    "\n",
    "K=1358\n",
    "468.17196463057627"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = g_walk.number_of_nodes()\n",
    "sampled_nodes = random.sample(list(g_walk.nodes), k)\n",
    "sampled_graph = g_walk.subgraph(sampled_nodes)\n",
    "\n",
    "all = nx.multi_source_dijkstra_path_length(g_walk, sampled_nodes, cutoff=20, weight=\"length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = list(all.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_public = copy.deepcopy(g_walk)\n",
    "\n",
    "public_nodes_n = g_public.number_of_nodes()\n",
    "ks = [\n",
    "    int(public_nodes_n * 0.1), # 10%\n",
    "    int(public_nodes_n * 0.2), # 20%\n",
    "    int(public_nodes_n * 0.3), # 30%\n",
    "    int(public_nodes_n * 0.4), # 40%\n",
    "    int(public_nodes_n * 0.5), # 50%\n",
    "    int(public_nodes_n * 0.6), # 60%\n",
    "    int(public_nodes_n * 0.7), # 70%\n",
    "    int(public_nodes_n * 0.8), # 80%\n",
    "    int(public_nodes_n * 0.9), # 90%\n",
    "]\n",
    "\n",
    "for k in ks:\n",
    "    sampled_nodes = random.sample(list(g_public.nodes), k)\n",
    "    sampled_graph = g_public.subgraph(sampled_nodes)\n",
    "\n",
    "    for node_id, data in sampled_graph.nodes(data=True):\n",
    "        print(data)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_result_path = f'{extractor.DATA_BASE_PATH}/results'\n",
    "\n",
    "def distance(p1, p2):\n",
    "    return geopy_distance.distance(p1, p2)\n",
    "    \n",
    "# conversion from lat/lng to x/y\n",
    "transform = pyproj.Transformer.from_crs(\"EPSG:4326\", \"EPSG:3857\", always_xy=True).transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.cluster\n",
    "\n",
    "\n",
    "def generic_stats(input_g: nx.MultiGraph | nx.MultiDiGraph, area: float):\n",
    "    # metrics for walking\n",
    "    circuity = 0\n",
    "    straightness = 0\n",
    "    euclidean_sum = 0\n",
    "    network_sum = 0\n",
    "    edges_len = input_g.number_of_edges()\n",
    "    elevation_sum = 0\n",
    "    \n",
    "    for u,v,data in input_g.edges(data=True):\n",
    "        p1 = (input_g._node[u][\"geometry\"].coords[0][1], input_g._node[u][\"geometry\"].coords[0][0])\n",
    "        p2 = (input_g._node[v][\"geometry\"].coords[0][1], input_g._node[v][\"geometry\"].coords[0][0])\n",
    "        euclidean_sum += distance(p1,p2).meters\n",
    "\n",
    "        elevation_sum += data[\"grade_abs\"]\n",
    "    \n",
    "        if \"length\" in data:\n",
    "            network_sum += data[\"length\"]\n",
    "        else:\n",
    "            network_sum += ops.transform(transform, data[\"geometry\"]).length\n",
    "    \n",
    "    print(\"circuity\")\n",
    "    circuity = network_sum / euclidean_sum\n",
    "    print(\"straightness\")\n",
    "    straightness = euclidean_sum / network_sum\n",
    "    print(\"orientation_entropy\")\n",
    "    orientation_entropy = ox.bearing.orientation_entropy(input_g)\n",
    "    print(\"road_density\")\n",
    "    road_density = network_sum / area\n",
    "    print(\"average_steepness\")\n",
    "    average_steepness = elevation_sum / edges_len\n",
    "    print(\"avg_street_length\")\n",
    "    avg_street_lenght = network_sum / edges_len\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"circuity\": circuity,\n",
    "        \"straightness\": straightness,\n",
    "        \"orientation_entropy\": orientation_entropy,\n",
    "        \"road_density\": road_density,\n",
    "        \"avg_steepness\": average_steepness,\n",
    "        \"avg_street_length\": avg_street_lenght\n",
    "    }\n",
    "\n",
    "def data_has_sidewalk(data):\n",
    "    if \"sidewalk\" in data:\n",
    "        if data[\"sidewalk\"] is not None:\n",
    "            if data[\"sidewalk\"] == \"both\" or data[\"sidewalk\"] == \"separate\" or data[\"sidewalk\"] == \"left\" or data[\"sidewalk\"] == \"right\" or data[\"sidewalk\"] == \"yes\":\n",
    "                return True\n",
    "            \n",
    "    if \"sidewalk:both\" in data:\n",
    "        if data[\"sidewalk:both\"] is not None:\n",
    "            if data[\"sidewalk:both\"] == \"separate\" or data[\"sidewalk:both\"] == \"yes\":\n",
    "                return True\n",
    "\n",
    "    if \"sidewalk:left\" in data:\n",
    "        if data[\"sidewalk:left\"] is not None:\n",
    "            if data[\"sidewalk:left\"] == \"separate\" or data[\"sidewalk:left\"] == \"yes\":\n",
    "                return True\n",
    "        \n",
    "    if \"sidewalk:right\" in data:\n",
    "        if data[\"sidewalk:right\"] is not None:\n",
    "            if data[\"sidewalk:right\"] == \"separate\" or data[\"sidewalk:right\"] == \"yes\":\n",
    "                return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "\n",
    "def calculate_walking_metrics(walk_g: nx.MultiGraph | nx.MultiDiGraph, drive_g: nx.MultiGraph | nx.MultiDiGraph, area: float):\n",
    "    # generalizer = generalize.Generalize()\n",
    "    # natural_streets_walking_graph = generalizer.named_streets_generalization(walk_g)\n",
    "    \n",
    "    walk_number_of_edges = walk_g.number_of_edges()\n",
    "    # drive_number_of_edges = drive_g.number_of_edges()\n",
    "    # walk_drive_ratio = walk_number_of_edges / drive_number_of_edges\n",
    "\n",
    "    # connectivity = []\n",
    "    # for node_id, data in natural_streets_walking_graph.nodes(data=True):\n",
    "    #     edges_len = len(natural_streets_walking_graph[node_id])\n",
    "    #     connectivity.append(edges_len)\n",
    "\n",
    "    # connectivity_array = np.array(connectivity)\n",
    "    # connectivity_mean = np.mean(connectivity_array)\n",
    "    # connectivity_std = np.std(connectivity_array)\n",
    "    # connectivity_range = connectivity_array.max() - connectivity_array.min()\n",
    "    # connectivity_90 = (connectivity_range / 10) * 9\n",
    "    # connectivity_top_10_len = len(list(filter(lambda x: x > connectivity_90, connectivity_array)))\n",
    "\n",
    "    # intersection count for undirected graph\n",
    "    intersections = 0\n",
    "    for node_id in walk_g.nodes():\n",
    "        if len(walk_g[node_id]) >= 3:\n",
    "            intersections += 1\n",
    "\n",
    "    road_scores_count = 0\n",
    "    for u,v,data in walk_g.edges(data=True):\n",
    "        road_score = 0\n",
    "\n",
    "        road_type = data[\"highway\"]\n",
    "        try:\n",
    "            max_speed = int(data[\"max_speed\"])\n",
    "        except: max_speed = None\n",
    "        has_sidewalk = data_has_sidewalk(data)\n",
    "        is_sidewalk = (data[\"footway\"] == \"sidewalk\") or (road_type == \"footway\")\n",
    "\n",
    "        if is_sidewalk or road_type == \"path\": \n",
    "            road_score = 5\n",
    "        elif \"trunk\" in road_type and not has_sidewalk:\n",
    "            road_score = 0\n",
    "        elif \"primary\" in road_type and not has_sidewalk:\n",
    "            road_score = 1\n",
    "        elif \"secondary\" in road_type and not has_sidewalk:\n",
    "            road_score = 2\n",
    "        elif \"tertiary\" in road_type:\n",
    "            road_score = 3\n",
    "        elif road_type == \"residential\" or road_type == \"living_street\":\n",
    "            road_score = 4\n",
    "        else: \n",
    "            road_score = 3\n",
    "        \n",
    "        # Improve rating if it has a sidewalk (not mapped separately)\n",
    "        if has_sidewalk:\n",
    "            road_score = (road_score + 2) % 5\n",
    "\n",
    "        # decrease score depending on road speed\n",
    "        if max_speed is not None:\n",
    "            if max_speed >= 30:\n",
    "                road_score -= 0.5\n",
    "            elif max_speed >= 50:\n",
    "                road_score -= 1\n",
    "            elif max_speed >= 80:\n",
    "                road_score -= 2\n",
    "            \n",
    "            if road_score < 0: road_score = 0\n",
    "\n",
    "        # sum of road scores\n",
    "        road_scores_count += road_score\n",
    "\n",
    "\n",
    "    mean_road_score = road_scores_count / walk_number_of_edges\n",
    "    # intersection_density = intersections / area\n",
    "    return {\n",
    "        # \"walk_connectivity_mean\": connectivity_mean,\n",
    "        # \"walk_connectivity_std\": connectivity_std,\n",
    "        # \"walk_connectivity_top_10p\": connectivity_top_10_len\n",
    "        \"walk_connectivity_mean\": 0,\n",
    "        \"walk_connectivity_std\": 0,\n",
    "        \"walk_connectivity_top_10p\": 0,\n",
    "\n",
    "        \"mean_road_score\": mean_road_score,\n",
    "        # \"walk_connectivity_mean\": connectivity_mean,\n",
    "        # \"walk_connectivity_std\": connectivity_std,\n",
    "        # \"walk_connectivity_top_10p\": connectivity_top_10_len\n",
    "        \"walk_connectivity_mean\": 0,\n",
    "        \"walk_connectivity_std\": 0,\n",
    "        \"walk_connectivity_top_10p\": 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_biking_metrics(bike_g: nx.MultiGraph | nx.MultiDiGraph, drive_g: nx.MultiGraph | nx.MultiDiGraph, area: float):\n",
    "    # generalizer = generalize.Generalize()\n",
    "    # natural_streets_bike_graph = generalizer.named_streets_generalization(bike_g)\n",
    "    # print(natural_streets_bike_graph.number_of_edges())\n",
    "    # print(natural_streets_bike_graph.number_of_nodes())\n",
    "    \n",
    "    bike_number_of_edges = bike_g.number_of_edges()\n",
    "    # drive_number_of_edges = drive_g.number_of_edges()\n",
    "    # bike_drive_ratio = bike_number_of_edges / drive_number_of_edges\n",
    "\n",
    "    # connectivity = []\n",
    "    # for node_id, data in natural_streets_bike_graph.nodes(data=True):\n",
    "    #     edges_len = len(natural_streets_bike_graph[node_id])\n",
    "    #     connectivity.append(edges_len)\n",
    "\n",
    "    # connectivity_array = np.array(connectivity)\n",
    "    # connectivity_mean = np.mean(connectivity_array)\n",
    "    # connectivity_std = np.std(connectivity_array)\n",
    "    # connectivity_range = connectivity_array.max() - connectivity_array.min()\n",
    "    # connectivity_90 = (connectivity_range / 10) * 9\n",
    "    # connectivity_top_10_len = len(list(filter(lambda x: x > connectivity_90, connectivity_array)))\n",
    "\n",
    "    # intersection count for undirected graph\n",
    "    # intersections = 0\n",
    "    # for node_id in bike_g.nodes():\n",
    "    #     if len(bike_g[node_id]) >= 3:\n",
    "    #         intersections += 1\n",
    "\n",
    "    road_scores_count = 0\n",
    "    for u,v,data in bike_g.edges(data=True):\n",
    "        road_score = 0\n",
    "        road_type = data[\"highway\"]\n",
    "        try:\n",
    "            max_speed = int(data[\"max_speed\"])\n",
    "        except: max_speed = None\n",
    "        if \"cycleway\" in data:\n",
    "            cycleway = data[\"cycleway\"]\n",
    "        else: \n",
    "            cycleway = None\n",
    "        inclination = data[\"grade_abs\"]\n",
    "        is_cycleway = road_type == \"cycleway\"\n",
    "\n",
    "        if is_cycleway: \n",
    "            road_score = 5\n",
    "        elif cycleway is not None:\n",
    "            if cycleway != \"no\": #includes lane\n",
    "                road_score = 4\n",
    "        elif \"trunk\" in road_type:\n",
    "            road_score = 0\n",
    "        elif \"primary\" in road_type:\n",
    "            road_score = 1\n",
    "        elif \"secondary\" in road_type:\n",
    "            road_score = 2\n",
    "        elif \"tertiary\" in road_type:\n",
    "            road_score = 3\n",
    "        elif road_type == \"residential\" or road_type == \"living_street\":\n",
    "            road_score = 4\n",
    "        else: \n",
    "            road_score = 3\n",
    "\n",
    "        # decrease score depending on road speed\n",
    "        if max_speed is not None:\n",
    "            if max_speed >= 50:\n",
    "                road_score -= 1\n",
    "            elif max_speed >= 80:\n",
    "                road_score -= 2\n",
    "            \n",
    "            if road_score < 0: road_score = 0\n",
    "\n",
    "        # decrease score depending on inclination\n",
    "        if inclination is not None:\n",
    "            inclination_percentage = inclination * 100\n",
    "            if inclination_percentage < 4: #easy inclination\n",
    "                road_score -= 0\n",
    "            elif inclination_percentage >= 4 and inclination_percentage < 7: # moderate inclination\n",
    "                road_score -= 1\n",
    "            elif inclination_percentage >= 7 and inclination_percentage < 9: # challenging\n",
    "                road_score -= 2\n",
    "            elif inclination_percentage > 9: # hard\n",
    "                road_score -= 3\n",
    "        \n",
    "        if road_score < 0: road_score = 0\n",
    "\n",
    "        # sum of road scores\n",
    "        road_scores_count += road_score\n",
    "\n",
    "\n",
    "    mean_road_score = road_scores_count / bike_number_of_edges\n",
    "    # intersection_density = intersections / area\n",
    "    return {\n",
    "        # \"bike_intersections_count\": intersections,\n",
    "        # \"bike_intersections_density\": intersection_density,\n",
    "        # \"bike_drive_ratio\": bike_drive_ratio,\n",
    "        \"bike_intersections_count\": 0,\n",
    "        \"bike_intersections_density\": 0,\n",
    "        \"bike_drive_ratio\": 0,\n",
    "        \"mean_road_score\": mean_road_score,\n",
    "        # \"bike_connectivity_mean\": connectivity_mean,\n",
    "        # \"bike_connectivity_std\": connectivity_std,\n",
    "        # \"bike_connectivity_top_10p\": connectivity_top_10_len\n",
    "        \"bike_connectivity_mean\": 0,\n",
    "        \"bike_connectivity_std\": 0,\n",
    "        \"bike_connectivity_top_10p\": 0\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"ottawa\"\n",
    "# #drive_graph = extractor.load_graph(f\"{folder}/graph/{folder}_drive\")\n",
    "# #drive_graph = extractor.load_graph(f\"{folder}/graph/{folder}_walk\")\n",
    "#bike_graph = extractor.load_graph(f\"{folder}/graph/{folder}_bike\")\n",
    "walk_graph = extractor.load_graph(f\"{folder}/graph/{folder}_walk\")\n",
    "extractor.save_as_shp(walk_graph, f\"{folder}/shp/{folder}_walk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen = Generalize()#\n",
    "# small = gen.named_streets_generalization(walk_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_result(results_dict, column, value):\n",
    "    results_dict[column].append(value)\n",
    "\n",
    "def clean_results(results_dict, max_index):\n",
    "    for idx in results_dict.keys():\n",
    "        results_dict[idx] = results_dict[idx][0:max_index]\n",
    "\n",
    "for folder, city_info in cities.items():\n",
    "\n",
    "    results_dict = {\n",
    "        \"city\": [],\n",
    "        \"area\": [],\n",
    "        \"built_up_area\": [],\n",
    "        \"population\": [],\n",
    "        \"pop_density\": [],\n",
    "\n",
    "        \"walk_circuity\": [],\n",
    "        \"walk_straightness\": [],\n",
    "        \"walk_orientation_entropy\": [],\n",
    "        \"walk_road_density\": [],\n",
    "        \"walk_avg_steepness\": [],\n",
    "        \"walk_avg_street_length\": [],\n",
    "        \"walk_intersections_count\": [],\n",
    "        \"walk_intersections_density\": [],\n",
    "        \"walk_drive_ratio\": [],\n",
    "        \"walk_mean_road_score\": [],\n",
    "        \"walk_connectivity_mean\": [],\n",
    "        \"walk_connectivity_std\": [],\n",
    "        \"walk_connectivity_top_10p\": [],\n",
    "\n",
    "        \"bike_circuity\": [],\n",
    "        \"bike_straightness\": [],\n",
    "        \"bike_orientation_entropy\": [],\n",
    "        \"bike_road_density\": [],\n",
    "        \"bike_avg_steepness\": [],\n",
    "        \"bike_avg_street_length\": [],\n",
    "        \"bike_intersections_count\": [],\n",
    "        \"bike_intersections_density\": [],\n",
    "        \"bike_drive_ratio\": [],\n",
    "        \"bike_mean_road_score\": [],\n",
    "        \"bike_connectivity_mean\": [],\n",
    "        \"bike_connectivity_std\": [],\n",
    "        \"bike_connectivity_top_10p\": []\n",
    "    }\n",
    "    results_dict = {\n",
    "        \"city\": [],\n",
    "        \"area\": [],\n",
    "        \"built_up_area\": [],\n",
    "        \"population\": [],\n",
    "        \"pop_density\": [],\n",
    "\n",
    "        \"walk_mean_road_score\": [],\n",
    "\n",
    "        \"bike_mean_road_score\": [],\n",
    "    }\n",
    "\n",
    "    print(f\"start {folder}\")\n",
    "    try:\n",
    "        total_area = city_stats.loc[city_stats[\"city\"] == city_info[\"search\"]][\"area\"].values[0]\n",
    "        area = city_stats.loc[city_stats[\"city\"] == city_info[\"search\"]][\"built_up_area\"].values[0]\n",
    "        population = city_stats.loc[city_stats[\"city\"] == city_info[\"search\"]][\"population\"].values[0]\n",
    "        population_density = population / area\n",
    "        add_result(results_dict, \"city\", folder)\n",
    "        add_result(results_dict, \"area\", total_area)\n",
    "        add_result(results_dict, \"built_up_area\", area)\n",
    "        add_result(results_dict, \"population\", population)\n",
    "        add_result(results_dict, \"pop_density\", population_density)\n",
    "    \n",
    "        print(\"loading graphs\")\n",
    "        walk_graph = extractor.load_graph(f\"{folder}/graph/{folder}_walk\")\n",
    "        # drive_graph = extractor.load_graph(f\"{folder}/graph/{folder}_drive\")\n",
    "        bike_graph = extractor.load_graph(f\"{folder}/graph/{folder}_bike\")\n",
    "        print(\"loaded graphs\")\n",
    "\n",
    "        print(\"walking metrics\")\n",
    "        # walking_metrics = calculate_walking_metrics(walk_graph, drive_graph, area)\n",
    "        walking_metrics = calculate_walking_metrics(walk_graph, None, area)\n",
    "        print(\"end walking metrics\")\n",
    "\n",
    "        print(\"biking metrics\")\n",
    "        # biking_metrics = calculate_biking_metrics(bike_graph, drive_graph, area)\n",
    "        biking_metrics = calculate_biking_metrics(bike_graph, None, area)\n",
    "        print(\"end biking metrics\")\n",
    "    \n",
    "        # print(\"calculating stats for walking graph\")\n",
    "        # walking_stats = generic_stats(walk_graph, area)\n",
    "        # print(\"calculating stats for bike graph\")\n",
    "        # bike_stats = generic_stats(bike_graph, area)\n",
    "    \n",
    "        #add_result(results_dict, \"walk_circuity\", walking_stats[\"circuity\"])\n",
    "        #add_result(results_dict, \"walk_straightness\", walking_stats[\"straightness\"])\n",
    "        #add_result(results_dict, \"walk_orientation_entropy\", walking_stats[\"orientation_entropy\"])\n",
    "        #add_result(results_dict, \"walk_road_density\", walking_stats[\"road_density\"])\n",
    "        #add_result(results_dict, \"walk_avg_steepness\", walking_stats[\"avg_steepness\"])\n",
    "        #add_result(results_dict, \"walk_avg_street_length\", walking_stats[\"avg_street_length\"])\n",
    "        #add_result(results_dict, \"walk_intersections_count\", walking_metrics[\"walk_intersections_count\"])\n",
    "        #add_result(results_dict, \"walk_intersections_density\", walking_metrics[\"walk_intersections_density\"])\n",
    "        #add_result(results_dict, \"walk_drive_ratio\", walking_metrics[\"walk_drive_ratio\"])\n",
    "        add_result(results_dict, \"walk_mean_road_score\", walking_metrics[\"mean_road_score\"])\n",
    "        #add_result(results_dict, \"walk_connectivity_mean\", walking_metrics[\"walk_connectivity_mean\"])\n",
    "        #add_result(results_dict, \"walk_connectivity_std\", walking_metrics[\"walk_connectivity_std\"])\n",
    "        #add_result(results_dict, \"walk_connectivity_top_10p\", walking_metrics[\"walk_connectivity_top_10p\"])\n",
    "\n",
    "        #add_result(results_dict, \"bike_circuity\", bike_stats[\"circuity\"])\n",
    "        #add_result(results_dict, \"bike_straightness\", bike_stats[\"straightness\"])\n",
    "        #add_result(results_dict, \"bike_orientation_entropy\", bike_stats[\"orientation_entropy\"])\n",
    "        #add_result(results_dict, \"bike_road_density\", bike_stats[\"road_density\"])\n",
    "        #add_result(results_dict, \"bike_avg_steepness\", bike_stats[\"avg_steepness\"])\n",
    "        #add_result(results_dict, \"bike_avg_street_length\", bike_stats[\"avg_street_length\"])\n",
    "        #add_result(results_dict, \"bike_intersections_count\", biking_metrics[\"bike_intersections_count\"])\n",
    "        #add_result(results_dict, \"bike_intersections_density\", biking_metrics[\"bike_intersections_density\"])\n",
    "        #add_result(results_dict, \"bike_drive_ratio\", biking_metrics[\"bike_drive_ratio\"])\n",
    "        add_result(results_dict, \"bike_mean_road_score\", biking_metrics[\"mean_road_score\"])\n",
    "        #add_result(results_dict, \"bike_connectivity_mean\", biking_metrics[\"bike_connectivity_mean\"])\n",
    "        #add_result(results_dict, \"bike_connectivity_std\", biking_metrics[\"bike_connectivity_std\"])\n",
    "        #add_result(results_dict, \"bike_connectivity_top_10p\", biking_metrics[\"bike_connectivity_top_10p\"])\n",
    "\n",
    "        results_df = pd.DataFrame(results_dict)\n",
    "        #results_df.to_csv(f'{metrics_result_path}/results2.csv')\n",
    "        try:\n",
    "            results_df_old = pd.read_csv(f'{metrics_result_path}/results_temp.csv', index_col=0)\n",
    "            results_df_new = pd.concat([results_df_old, results_df]).reset_index(drop=True)\n",
    "            results_df_new.to_csv(f'{metrics_result_path}/results_temp.csv')\n",
    "        except:\n",
    "            results_df.to_csv(f'{metrics_result_path}/results_temp.csv')\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        print(f\"Error in city {folder}\")\n",
    "    print(\"-------------------------------------------------------------\")\n",
    "    \n",
    "results_df_final = pd.read_csv(f'{metrics_result_path}/results_temp.csv', index_col=0)\n",
    "results_df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_old = pd.read_csv(f'{metrics_result_path}/results_temp.csv', index_col=0).reset_index(drop=True)\n",
    "results_2 = pd.read_csv(f'{metrics_result_path}/results2.csv', index_col=0).reset_index(drop=True)\n",
    "\n",
    "results_2[\"walk_mean_road_score\"] = results_df_old[\"walk_mean_road_score\"]\n",
    "results_2[\"bike_mean_road_score\"] = results_df_old[\"bike_mean_road_score\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_old = pd.read_csv(f'{metrics_result_path}/results.csv', index_col=0).reset_index(drop=True)\n",
    "results_df_old[[\"city\",\"walk_mean_road_score\",\"bike_mean_road_score\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_final[[\"city\", \"bike_avg_steepness\", \"walk_road_density\", \"bike_avg_street_length\", \"bike_intersections_count\", \"bike_intersections_density\", \"bike_connectivity_mean\", \"bike_connectivity_top_10p\"]]\n",
    "\n",
    "common_cols = ['city', 'area', 'built_up_area', 'population', 'pop_density']\n",
    "walk_cols = common_cols + list(filter(lambda x: str(x).find(\"walk\") != -1, results_columns))\n",
    "bike_cols = common_cols + list(filter(lambda x: str(x).find(\"bike\") != -1, results_columns))\n",
    "\n",
    "print(walk_cols)\n",
    "print(bike_cols)\n",
    "\n",
    "walk_results = results_df_final[walk_cols]\n",
    "bike_results = results_df_final[bike_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ox_results = pd.read_csv(f'/home/geolab/Downloads/indicators.csv')\n",
    "ox_results.loc[ox_results[\"core_city\"].str.contains(\"lagos\")][[\"built_up_area\",\"area\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.read_csv(f'{extractor.DATA_BASE_PATH}/stats.csv', index_col=0)\n",
    "stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
